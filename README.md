# MLP Autoencoder From Scratch

Implement multilayer perceptrons (MLP) from scratch that contains a hidden layer and tanh activation function(only hidden layer) with 64 input, 2 hidden, and 64 output units. Train the network using stochastic gradient descent with mini-batches and use mean-square-error (MSE) as the loss function. Train your MLP for multiple epochs with optdigits.tra and visualize the output of the hidden layer on a scatter plot.
